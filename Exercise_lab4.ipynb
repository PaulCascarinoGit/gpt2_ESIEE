{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab88d307-61ba-45ef-89bc-e3569443dfca",
   "metadata": {},
   "source": [
    "# Chapter 4 - Lab 4 - Exercise\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "> Response by Paul CASCARINO E5-DSIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {},
   "source": [
    "# Exercise 4.1: Parameters in the feed forward versus attention module\n",
    "\n",
    "### How do the parameter counts differ between the `feed-forward` neural network module and `multi-head attention` mechanism in our transformer architecture?**\n",
    "\n",
    "#### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af73d944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
      "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
      "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
      "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
      "\n",
      "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
      "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
      "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
      "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import matplotlib\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "# Previous lab\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        #assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d45320",
   "metadata": {},
   "source": [
    "#### 1. Enumerate parameters in `feed-forward` module\n",
    "\n",
    "The `feed-forward` module is composed of 2 linear layers and an intermediate activation function (ReLU or GeLU). We use the code below to implement that part in the lab : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67c5403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b04a90",
   "metadata": {},
   "source": [
    "As we can see, the total number of parameters ($n_{par}$) in `feed-forward` module can be obtained with : \n",
    "\n",
    "- First Layer : $n_{par1} = Weights + bias = emb_{dim} * 4 * emb_{dim} + 4 *emb_{dim} = 4*emb_{dim}^2+4*emb_{dim}$\n",
    "\n",
    "- Second Layer : $n_{par2} = Weights + bias = 4 * emb_{dim} * emb_{dim} + emb_{dim} = 4*emb_{dim}^2+emb_{dim}$\n",
    "\n",
    "- Total : $n_{par} = 8*emb_{dim}^2+5*emb_{dim}$\n",
    "\n",
    "We can have $emb_{dim}$ with the configuration code below : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96969d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7fb094",
   "metadata": {},
   "source": [
    "So, the total number of parameters ($n_{par}$) in `feed-forward` module is 4 722 732:\n",
    "\n",
    "$n_{par} = 8*768^2+5*768 = 4722732$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f72d7f",
   "metadata": {},
   "source": [
    "#### 2. Enumerate parameters in `multi-head attention` module\n",
    "\n",
    "The `multi-head attention` module is composed of 3 linear layers and a Output Projection layer. We use the code below to implement that part in the lab : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e77c3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "\n",
    "#         #...\n",
    "\n",
    "#         self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "#         self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "#         #..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c07a14a",
   "metadata": {},
   "source": [
    "As we can see, the total number of parameters ($n_{par}$) in `multi-head attention` module can be obtained with : \n",
    "\n",
    "- 3 First Layer : $n_{par1} = 3*(d_{in}*d_{out} + d_{out})=3*d_{in}*d_{out}+3*d_{out}$\n",
    "\n",
    "- Output Projection layer : $n_{par2} = Weights + bias = d_{out}*d_{out}+d_{out}=d_{out}^2+d_{out}$\n",
    "\n",
    "- Total : $n_{par} = 3*d_{in}*d_{out}+3*d_{out} + d_{out}^2+d_{out} = d_{out}^2 + 3*d_{in}*d_{out} + 4*d_{out}$\n",
    "\n",
    "We can have $d_{out}$ and $d_{in}$ with the code below : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59c579e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, cfg):\n",
    "#         super().__init__()\n",
    "#         self.att = MultiHeadAttention(\n",
    "#             d_in=cfg[\"emb_dim\"],\n",
    "#             d_out=cfg[\"emb_dim\"],\n",
    "#             ...\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85000e7d",
   "metadata": {},
   "source": [
    "So, the total number of parameters ($n_{par}$) in `multi-head attention` module is 879 168:\n",
    "\n",
    "$n_{par} = 768^2 + 3*768*768 + 4*768$ = 4*768^2+4*768 = 879168\n",
    "\n",
    "#### 3. Perform comparative statistical analysis\n",
    "\n",
    "\n",
    "To conclude, we have $n_{par feed-forward } = 4 722 732$ $n_{par multi-head attention } = 879 168$.\n",
    "\n",
    "It implies that  ($n_{par}$) in our example multi-head attention` is around 5.3 times less important than ($n_{par}$) in `feed-forward` and require around 5.3 times less ressources to perform\n",
    "\n",
    "\n",
    "#### 4. Interpret parametric distribution characteristics\n",
    "\n",
    "We have : \n",
    "\n",
    "- $n_{par feed-forward } = 8*emb_{dim}^2+5*emb_{dim}$ whose have \n",
    "  - a quadratic term 8*emb_{dim}^2\n",
    "  - a linear term 5*emb_{dim}\n",
    "\n",
    "- $n_{par multi-head attention } = d_{out}^2 + 3*d_{in}*d_{out} + 4*d_{out}$ with $d_{out}=d_{in}=emb_{dim}$\n",
    "  - a quadratic term 4*emb_{dim}^2\n",
    "  - a linear term 4*emb_{dim}\n",
    "\n",
    "The ratio is $(8*emb_{dim}^2+5*emb_{dim}) / (4*emb_{dim}^2+4*emb_{dim}) \\simeq 8/4= 2$\n",
    "\n",
    "`feed-forward` modules are more parameter-heavy, implying they require more memory and computational resources for training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0b7fb9",
   "metadata": {},
   "source": [
    "# Exercise 4.2: Initialize larger GPT models\n",
    "\n",
    "### Can you systematically scale the GPT-2 model architecture from the small configuration to medium, large, and XL variants by exclusively modifying the configuration parameters?\n",
    "\n",
    "#### 1. GPT-2 Small (Current Implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d64d3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_small = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_small)\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb85302",
   "metadata": {},
   "source": [
    "$n_{tot} = vocab_{size}*context_{length} + n_{layers}*((4+8)*emb_{dim}^2+(4+5)*emb_{dim}) + vocab_{size}*context_{length}$\n",
    "\n",
    "\n",
    "- With counting the of output layer : \n",
    "\n",
    "  $n_{tot} = 2*vocab_{size}*context_{length} + n_{layers}*(12*emb_{dim}^2+9*emb_{dim})$\n",
    "\n",
    "  $n_{tot small} = 2*50257*768 + 12*(12*768^2+9*768) = 162212352  \\simeq 162M$\n",
    "\n",
    "- Without counting the of output layer : \n",
    "\n",
    "  $n_{tot} = 1*vocab_{size}*context_{length} + n_{layers}*(12*emb_{dim}^2+9*emb_{dim})$\n",
    "\n",
    "  $n_{tot small} = 1*50257*768 + 12*(12*768^2+9*768) = 123614976  \\simeq 123M$\n",
    "\n",
    "  The number of parameter of GPT2 small is 162M counting the of output layer 123M otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87641fd",
   "metadata": {},
   "source": [
    "#### 2. GPT-2 Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aef4e5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 305,467,392\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_medium= {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1024,         # Embedding dimension\n",
    "    \"n_heads\": 24,          # Number of attention heads\n",
    "    \"n_layers\": 16,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_medium)\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519589f",
   "metadata": {},
   "source": [
    "- With counting the of output layer : \n",
    "\n",
    "  $n_{tot} = 2*vocab_{size}*context_{length} + n_{layers}*(12*emb_{dim}^2+9*emb_{dim})$\n",
    "\n",
    "  $n_{tot small} = 2*50257*1024 + 24*(12*1024^2+9*1024) = 405 137 408  \\simeq 405M$\n",
    "\n",
    "- Without counting the of output layer : \n",
    "\n",
    "  $n_{tot} = 1*vocab_{size}*context_{length} + n_{layers}*(12*emb_{dim}^2+9*emb_{dim})$\n",
    "\n",
    "  $n_{tot small} = 1*50257*1024 + 24*(12*1024^2+9*1024) = 353 674 240  \\simeq 353M$\n",
    "\n",
    "  The number of parameter of GPT2 medium is 405M counting the of output layer 353M otherwise\n",
    "\n",
    "\n",
    "#### 3. GPT-2 Large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4e799ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 523,443,200\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_large = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1280,         # Embedding dimension\n",
    "    \"n_heads\": 36,          # Number of attention heads\n",
    "    \"n_layers\": 20,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_large)\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874b9c2",
   "metadata": {},
   "source": [
    "- With counting the of output layer : \n",
    "\n",
    "  $n_{tot} = 2*vocab_{size}*context_{length} + n_{layers}*(12*emb_{dim}^2+9*emb_{dim})$\n",
    "\n",
    "  $n_{tot small} = 2*50257*1280 + 36*(12*1280^2+9*1280) = 836 861 440  \\simeq 836M$\n",
    "\n",
    "- Without counting the of output layer : \n",
    "\n",
    "  $n_{tot} = 1*vocab_{size}*context_{length} + n_{layers}*(12*emb_{dim}^2+9*emb_{dim})$\n",
    "\n",
    "  $n_{tot small} = 1*50257*1280 + 36*(12*1280^2+9*1280) = 772532480  \\simeq 772M$\n",
    "\n",
    "  The number of parameter of GPT2 large is 836M counting the of output layer 772M otherwise\n",
    "\n",
    "#### 4. GPT-2 XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67150775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 930,864,000\n"
     ]
    }
   ],
   "source": [
    "GPT_CONFIG_xl = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 1600,         # Embedding dimension\n",
    "    \"n_heads\": 48,          # Number of attention heads\n",
    "    \"n_layers\": 25,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_xl)\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e8bd",
   "metadata": {},
   "source": [
    "- With counting the of output layer : \n",
    "\n",
    "  $n_{tot} = 2*vocab_{size}*context_{length} + n_{layers}*(12*emb_{dim}^2+9*emb_{dim})$\n",
    "\n",
    "  $n_{tot small} = 2*50257*1600 + 48*(12*1600^2+9*1600) = 1 636 073 600  \\simeq 1636M$\n",
    "\n",
    "- Without counting the of output layer : \n",
    "\n",
    "  $n_{tot} = 1*vocab_{size}*context_{length} + n_{layers}*(12*emb_{dim}^2+9*emb_{dim})$\n",
    "\n",
    "  $n_{tot small} = 1*50257*1600 + 48*(12*1600^2+9*1600) = 1555662400  \\simeq 1555M$\n",
    "\n",
    "  The number of parameter of GPT2 extra large is 1636M counting the of output layer 1555M otherwise\n",
    "\n",
    "\n",
    "  To conclude :\n",
    "  \n",
    "  - $n{par M} > 2.5*n{par S}$ \n",
    "  - $n{par L} > 2*n{par M}$ \n",
    "  - $n{par XL} > 2*n{par L}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b3566",
   "metadata": {},
   "source": [
    "# Exercise 4.3: Using separate dropout parameters\n",
    "\n",
    "### How can we enhance the dropout configuration of the GPT model by implementing layer-specific dropout rates?\n",
    "\n",
    "#### 1. Replace the monolithic `drop_rate` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c1a8f",
   "metadata": {},
   "source": [
    "In the current implementation, a single dropout rate is used for the entire model via the *drop_rate* variable. \n",
    "\n",
    "However, this approach does not allow customization of the regularizations applied to the different components of the architecture.\n",
    "\n",
    "We will add in our config : \n",
    "\n",
    "- drop_emb : This will control the dropout rate for the embedding layer.\n",
    "\n",
    "- drop_residual : This will handle the dropout applied to the residual (shortcut) connections.\n",
    "\n",
    "- drop_attention : This will manage the dropout used specifically in the multi-head attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db7a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M_new = {\n",
    "    \"vocab_size\": 50257,        # Vocabulary size\n",
    "    \"context_length\": 1024,     # Context length\n",
    "    \"emb_dim\": 768,             # Embedding dimension\n",
    "    \"n_heads\": 12,              # Number of attention heads\n",
    "    \"n_layers\": 12,             # Number of layers\n",
    "    \"drop_emb\": 0.1,            # Embedding dropout rate\n",
    "    \"drop_residual\": 0.1,       # Shortcut dropout rate\n",
    "    \"drop_attention\": 0.1,      # Attention dropout rate\n",
    "    \"qkv_bias\": False           # Query-Key-Value bias\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f43441",
   "metadata": {},
   "source": [
    "#### 2. Introduce a hierarchical dropout configuration and Maintain the overall structural integrity of the model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce173462",
   "metadata": {},
   "source": [
    "*drop_emb* applied in the embedding layers (tok_emb and pos_emb) in the GPTModel. \n",
    "\n",
    "We also pass he *drop_residual* and *drop_attention* parameters to the respective components in the TransformerBlock and MultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a3e1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_emb\"])  # Embedding dropout\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)         # Apply embedding dropout\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adeb9ac",
   "metadata": {},
   "source": [
    "*drop_residual*: Applied to the residual (shortcut) connections in the TransformerBlock.\n",
    "\n",
    "we need to pass the *drop_residual* parameter for the residual connections and *drop_attention* for the attention dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27e248fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_attention\"],  # Apply attention-specific dropout\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_residual = nn.Dropout(cfg[\"drop_residual\"])  # Apply residual dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_residual(x)  # Apply residual dropout\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_residual(x)  # Apply residual dropout\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc8d5d6",
   "metadata": {},
   "source": [
    "*drop_attention:* Applied to the attention weights in the MultiHeadAttention.\n",
    "\n",
    "We will pass *drop_attention* parameter for the attention-specific dropout within the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf749fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)  # Apply attention dropout\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
