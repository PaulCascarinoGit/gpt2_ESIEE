{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab88d307-61ba-45ef-89bc-e3569443dfca",
   "metadata": {},
   "source": [
    "# Chapter 3 - Exercises\n",
    "\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "> Response by Paul CASCARINO E5-DSIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {},
   "source": [
    "# Exercise 3.1\n",
    "\n",
    "### Can you transfer the weights from `SelfAttention_v2` to `SelfAttention_v1` such that both implementations produce identical output tensors?\n",
    "\n",
    "#### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d743b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.5085, 0.3508],\n",
      "        [0.5084, 0.3508],\n",
      "        [0.5084, 0.3506],\n",
      "        [0.5074, 0.3471],\n",
      "        [0.5076, 0.3446],\n",
      "        [0.5077, 0.3493]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    \n",
    "\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "#print(sa_v1(inputs))\n",
    "\n",
    "\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "#print(sa_v2(inputs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0bb0bd",
   "metadata": {},
   "source": [
    "#### 1. My responses\n",
    "\n",
    "- In `SelfAttention_v2`, the use of the PyTorch's Linear layers `nn.Linear` \"which are equivalent to a matrix multiplication if we disable the bias units\" (cf lab.3). So `nn.Linear` stores its training weight matrix $W_q$, $W_k$, and $W_v$ with a shape $[d_{out}, d_{in}]$ wich is a transposed configuration.\n",
    "\n",
    "-  In `SelfAttention_v1`, the training weight matrix $W_q$, $W_k$, and $W_v$ are a directly stored with the shape $[d_{in}, d_{out}]$ \n",
    "\n",
    "- It implies that to transfer the weights from `SelfAttention_v2` to `SelfAttention_v1` we have **transpose** them\n",
    "\n",
    "- \n",
    "\n",
    "#### 2. The implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83358b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from SelfAttention_v2: tensor([[0.5085, 0.3508],\n",
      "        [0.5084, 0.3508],\n",
      "        [0.5084, 0.3506],\n",
      "        [0.5074, 0.3471],\n",
      "        [0.5076, 0.3446],\n",
      "        [0.5077, 0.3493]], grad_fn=<MmBackward0>)\n",
      "Output from SelfAttention_v1: tensor([[0.5085, 0.3508],\n",
      "        [0.5084, 0.3508],\n",
      "        [0.5084, 0.3506],\n",
      "        [0.5074, 0.3471],\n",
      "        [0.5076, 0.3446],\n",
      "        [0.5077, 0.3493]], grad_fn=<MmBackward0>)\n",
      "Success: Outputs from both implementations match.\n"
     ]
    }
   ],
   "source": [
    "# Transfer weights from sa_v2 to sa_v1\n",
    "with torch.no_grad():\n",
    "    sa_v1.W_query.copy_(sa_v2.W_query.weight.T)\n",
    "    sa_v1.W_key.copy_(sa_v2.W_key.weight.T)\n",
    "    sa_v1.W_value.copy_(sa_v2.W_value.weight.T)\n",
    "\n",
    "# Verify outputs\n",
    "output_v2 = sa_v2(inputs)\n",
    "output_v1 = sa_v1(inputs)\n",
    "\n",
    "print(\"Output from SelfAttention_v2:\", output_v2)\n",
    "print(\"Output from SelfAttention_v1:\", output_v1)\n",
    "\n",
    "# Assert equivalence\n",
    "assert torch.allclose(output_v1, output_v2, atol=1e-6), \"Outputs do not match!\"\n",
    "print(\"Success: Outputs from both implementations match.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5d5d7",
   "metadata": {},
   "source": [
    "# Exercise 3.2\n",
    "\n",
    "### How can you modify the input arguments to the `MultiHeadAttentionWrapper(num_heads=2)` to transform the output context vectors from four-dimensional to two-dimensional while maintaining the `num_heads=2` configuration?\n",
    "\n",
    "#### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a115920c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "    \n",
    "    \n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3\n",
    "\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ffeb1",
   "metadata": {},
   "source": [
    "#### 1. My response\n",
    "\n",
    "- The input parameter that that controls the dimensionality of output context vectors in the `MultiHeadAttentionWrapper` is $d_{out}$ wich is passed to `CausalAttention` head. $d_{out}$ is the \"embedding dimension\" for the key, query and value vectors for each head.\n",
    "\n",
    "- It implies that the final embedding dimension is $d_{out} * num_{heads}$. If we maintain $num_{head} = 2$ the final embedding dimension is  $2 * d_{out}$\n",
    "\n",
    "- We want a two-dimensional output context vectors implies that $d_{out} = 1$\n",
    "\n",
    "- It implies that each attention head will output a scalar value per token\n",
    "\n",
    "- So we need to **aggregate the outputs** to achieve the 2-dimensional output. A solution is to **we can stack the outputs and average them along the head dimension**.\n",
    "\n",
    "#### 2. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b24d4",
   "metadata": {},
   "source": [
    "    def forward(self, x):\n",
    "\n",
    "        last_output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "        return torch.mean(last_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60af1179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 1])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        head_output = [head(x) for head in self.heads]\n",
    "\n",
    "        return torch.mean(torch.stack(head_output, dim=-1), dim=-1)\n",
    "    \n",
    "\n",
    "d_in, d_out = 3, 1  \n",
    "context_length = batch.shape[1]   # This is the number of tokens\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, dropout=0.0, num_heads=2\n",
    ")\n",
    "\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2279ca",
   "metadata": {},
   "source": [
    "# Exercise 3.3\n",
    "\n",
    "### Can you configure a `MultiHeadAttention` module that precisely replicates the architectural specifications of the smallest GPT-2 model?\n",
    "\n",
    "#### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d9ea5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "# Model configurations\n",
    "d_in = 768  # Input embedding dimension (also d_model for GPT-2)\n",
    "d_out = 768  # Output embedding dimension (same as input for GPT-2)\n",
    "num_heads = 12  # Number of attention heads in GPT-2 smallest model\n",
    "context_length = 1024  # The context length (number of tokens GPT-2 can process)\n",
    "dropout = 0.1  # Typical dropout rate for transformer models\n",
    "\n",
    "# Create an instance of the MultiHeadAttentionWrapper\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in=d_in, \n",
    "    d_out=d_out, \n",
    "    context_length=context_length, \n",
    "    dropout=dropout, \n",
    "    num_heads=num_heads\n",
    ")\n",
    "\n",
    "# Example input (batch size = 2, sequence length = 1024, embedding size = 768)\n",
    "inputs = torch.rand(2, context_length, d_in)\n",
    "\n",
    "# Forward pass\n",
    "context_vecs = mha(inputs)\n",
    "\n",
    "# Output shape should be [2, 1024, 768] as per GPT-2's architecture\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d4d91",
   "metadata": {},
   "source": [
    "Key Implementation Details:\n",
    "12 Parallel Attention Heads:\n",
    "\n",
    "The model has been configured with num_heads=12 in the MultiHeadAttentionWrapper, which ensures 12 attention heads are used. This allows the attention mechanism to capture multiple perspectives of the input sequence and to focus on different subspaces of the input representations.\n",
    "768-Dimensional Embedding Space:\n",
    "\n",
    "Both the input (d_in) and output (d_out) embedding dimensions are set to 768, which is consistent with GPT-2's architecture. This means that each token is represented in a 768-dimensional space, which facilitates the representation of rich contextual information.\n",
    "1,024 Token Context Length:\n",
    "\n",
    "The context_length=1024 setting allows the model to process sequences of up to 1,024 tokens. This is directly in line with the smallest GPT-2 model, which can handle 1,024 tokens in a single forward pass, enabling it to understand long-range dependencies in text.\n",
    "Practical Recommendation:\n",
    "The MultiHeadAttentionWrapper is initialized with the exact specifications needed to match GPT-2's smallest model:\n",
    "d_in and d_out set to 768 ensure that the attention mechanism operates with the appropriate dimensionality for both input and output.\n",
    "num_heads=12 ensures the correct number of attention heads, which allows for the multi-perspective learning characteristic of GPT-2.\n",
    "context_length=1024 ensures the model can process long sequences, aligning with the smallest GPT-2 model's context length.\n",
    "Thus, the provided configuration is correctly designed to replicate the smallest GPT-2 model’s attention mechanism.\n",
    "\n",
    "Outcome:\n",
    "By constructing the MultiHeadAttentionWrapper in this way, you’ve replicated the essential architectural features of GPT-2’s smallest model:\n",
    "\n",
    "12 attention heads for multi-perspective processing\n",
    "768-dimensional embedding space for token representations\n",
    "1,024 token context length for sequence processing\n",
    "This ensures that your MultiHeadAttention module matches GPT-2’s attention architecture accurately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
