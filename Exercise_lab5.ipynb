{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab88d307-61ba-45ef-89bc-e3569443dfca",
   "metadata": {},
   "source": [
    "# Chapter 5 - Lab 5 - Exercise\n",
    "> Author : Badr TAJINI - Large Language model (LLMs) - ESIEE 2024-2025\n",
    "\n",
    "> Response by Paul CASCARINO E5-DSIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f678e62-7bcb-4405-86ae-dce94f494303",
   "metadata": {},
   "source": [
    "#### 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af73d944",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 36\u001b[0m\n\u001b[0;32m     25\u001b[0m GPT_CONFIG_124M \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50257\u001b[39m,   \u001b[38;5;66;03m# Vocabulary size\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m256\u001b[39m, \u001b[38;5;66;03m# Shortened context length (orig: 1024)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqkv_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m      \u001b[38;5;66;03m# Query-key-value bias\u001b[39;00m\n\u001b[0;32m     33\u001b[0m }\n\u001b[0;32m     35\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGPT_CONFIG_124M\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39meval(); \n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_to_token_ids\u001b[39m(text, tokenizer):\n",
      "File \u001b[1;32mc:\\Users\\paulc\\Documents\\Esiee_Paris\\E5\\p2\\data_engineering\\Paul\\previous_labs.py:200\u001b[0m, in \u001b[0;36mGPTModel.__init__\u001b[1;34m(self, cfg)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrf_blocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;241m*\u001b[39m[TransformerBlock(cfg) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m])])\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm \u001b[38;5;241m=\u001b[39m LayerNorm(cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43memb_dim\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvocab_size\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:112\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:118\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\init.py:518\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[1;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[0;32m    516\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from previous_labs import GPTModel, generate_text_simple, create_dataloader_v1\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://huggingface.co/datasets/DarwinAnim8or/the-verdict/blob/main/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval(); \n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf06f22",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0004\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m     10\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m---> 11\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvery effort moves you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 134\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[1;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[0;32m    132\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Reset loss gradients from previous batch iteration\u001b[39;00m\n\u001b[0;32m    133\u001b[0m loss \u001b[38;5;241m=\u001b[39m calc_loss_batch(input_batch, target_batch, model, device)\n\u001b[1;32m--> 134\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Calculate loss gradients\u001b[39;00m\n\u001b[0;32m    135\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep() \u001b[38;5;66;03m# Update model weights using loss gradients\u001b[39;00m\n\u001b[0;32m    136\u001b[0m tokens_seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 2\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123) # Manual seed for reproducibility\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item() for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bcc1b0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 38.5kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 969kiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 30.1kiB/s]\n",
      "model.ckpt.data-00000-of-00001:  84%|████████▍ | 419M/498M [00:46<00:08, 9.24MiB/s] "
     ]
    }
   ],
   "source": [
    "# I will use the open ai model\n",
    "from gpt_download import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "\n",
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval();\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6b0c99",
   "metadata": {},
   "source": [
    "# Exercise 5.1: Temperature-scaled softmax scores and sampling probabilities\n",
    "\n",
    "### How does temperature-based scaling of the `softmax` probability distribution impact the sampling frequency of the specific lexical token `\"pizza\"`?\n",
    "\n",
    "#### 1. Primary token of interest: `\"pizza\"`\n",
    "\n",
    "##### 1.1 Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e4f2493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled tokens with temperature 0.1:\n",
      "0 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "992 x forward\n",
      "0 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "8 x toward\n",
      "\n",
      "Sampled tokens with temperature 1:\n",
      "71 x closer\n",
      "2 x every\n",
      "0 x effort\n",
      "544 x forward\n",
      "2 x inches\n",
      "1 x moves\n",
      "0 x pizza\n",
      "376 x toward\n",
      "4 x you\n",
      "\n",
      "Sampled tokens with temperature 2.0:\n",
      "158 x closer\n",
      "20 x every\n",
      "12 x effort\n",
      "391 x forward\n",
      "37 x inches\n",
      "5 x moves\n",
      "4 x pizza\n",
      "340 x toward\n",
      "33 x you\n",
      "\n",
      "Sampled tokens with temperature 5.0:\n",
      "153 x closer\n",
      "68 x every\n",
      "55 x effort\n",
      "223 x forward\n",
      "102 x inches\n",
      "50 x moves\n",
      "43 x pizza\n",
      "218 x toward\n",
      "88 x you\n",
      "\n",
      "Sampled tokens with temperature 10.0:\n",
      "142 x closer\n",
      "82 x every\n",
      "93 x effort\n",
      "168 x forward\n",
      "114 x inches\n",
      "83 x moves\n",
      "62 x pizza\n",
      "165 x toward\n",
      "91 x you\n"
     ]
    }
   ],
   "source": [
    "# Example: Apply temperature scaling and print sample frequencies for different temperatures\n",
    "temperature = 0.1\n",
    "probas = softmax_with_temperature(next_token_logits, temperature)\n",
    "print(f\"Sampled tokens with temperature {temperature}:\")\n",
    "print_sampled_tokens(probas)\n",
    "\n",
    "# Lower temperature (e.g., 0.1) makes the output more deterministic\n",
    "temperature = 1\n",
    "probas = softmax_with_temperature(next_token_logits, temperature)\n",
    "print(f\"\\nSampled tokens with temperature {temperature}:\")\n",
    "print_sampled_tokens(probas)\n",
    "\n",
    "# Higher temperature (e.g., 2.0) introduces more randomness\n",
    "temperature = 2.0\n",
    "probas = softmax_with_temperature(next_token_logits, temperature)\n",
    "print(f\"\\nSampled tokens with temperature {temperature}:\")\n",
    "print_sampled_tokens(probas)\n",
    "\n",
    "# Higher temperature (e.g., 5.0) introduces more randomness\n",
    "temperature = 5.0\n",
    "probas = softmax_with_temperature(next_token_logits, temperature)\n",
    "print(f\"\\nSampled tokens with temperature {temperature}:\")\n",
    "print_sampled_tokens(probas)\n",
    "\n",
    "# Higher temperature (e.g., 10.0) introduces more randomness\n",
    "temperature = 10.0\n",
    "probas = softmax_with_temperature(next_token_logits, temperature)\n",
    "print(f\"\\nSampled tokens with temperature {temperature}:\")\n",
    "print_sampled_tokens(probas)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb4444",
   "metadata": {},
   "source": [
    "The results show how temperature affects the sampling distribution. At a temperature of 0.1, the model's output is highly deterministic, with \"forward\" dominating the samples and other tokens like \"pizza\" being virtually never selected. At a temperature of 1.0, the model shows more balanced diversity, with \"closer\" and \"forward\" still dominant, but tokens like \"pizza\" start to appear, albeit infrequently. Finally, at a temperature of 2.0, the sampling becomes more random, allowing less frequent tokens like \"pizza\" to appear more often, though \"closer\" and \"forward\" remain frequent. This demonstrates how lower temperatures make the model more predictable, while higher temperatures increase diversity and randomness in token selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa01e405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature values\n",
    "temperatures = [0.1, 0.5, 1, 5, 10, 50] \n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "\n",
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e42c8",
   "metadata": {},
   "source": [
    "# Exercise 5.2: Different temperature and top-k settings\n",
    "\n",
    "### How do variations in `temperature` and `top-k` sampling parameters influence the qualitative and probabilistic characteristics of token generation in stochastic language models?\n",
    "\n",
    "\n",
    "- Temperature: The temperature parameter controls the randomness or creativity of the generated text.\n",
    "  - Low temperature (e.g., 0.1–0.5) results in more deterministic outputs, where the model picks higher-probability tokens.\n",
    "  - High temperature (e.g., 1.0–1.5) introduces more randomness, allowing the model to choose less likely tokens and thus generating more varied and creative outputs.\n",
    "\n",
    "- Top-k Sampling: Top-k sampling involves restricting token generation to the k most likely candidates at each step.\n",
    "  - Lower values of k (e.g., k=1) mean the model will always pick the most probable token (deterministic output).\n",
    "  - Higher values of k provide more variety in the generated text by selecting from a broader range of tokens.\n",
    "\n",
    "\n",
    "**We can see below results from different temperatures for our first \"trained\" model with low epochs to run it during hours:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28a4d8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Temperature values\n",
    "temperatures = [0.1, 0.5, 1, 5] \n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]\n",
    "\n",
    "\n",
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i], bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24536e",
   "metadata": {},
   "source": [
    "We can see that More the temperature is high, more the results will be various implies less probability in peak tha low temperature's results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c0089f",
   "metadata": {},
   "source": [
    "#### 1.1  Low `temperature` and `top-k` Scenarios\n",
    "\n",
    "Let's test some example with the OpenAI model because mine is not trained as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4df9dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very Low temp, Very Low top-k:\n",
      " Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work.\n",
      "\n",
      "The second step is to understand the importance of your work.\n",
      "\n",
      "The third step is to understand the importance of your work.\n",
      "\n",
      "The fourth step is\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"Very Low temp, Very Low top-k:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1c5f3301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low temp, Low top-k:\n",
      " Every effort moves you forward.\n",
      "\n",
      "The first step is to find a way to make the world a better place.\n",
      "\n",
      "This is not a new idea.\n",
      "\n",
      "In the past, the world was filled with people who were willing to work hard to make it\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=2,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "print(\"Low temp, Low top-k:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86f78f2",
   "metadata": {},
   "source": [
    "In the \"Very Low temp, Very Low top-k\" case, the output is highly repetitive, with the model continually focusing on the same theme: understanding the importance of work. This results in a rigid and repetitive structure without introducing any new ideas or variety. On the other hand, the \"Low temp, Low top-k\" output introduces more variation and a slightly broader perspective, shifting the focus towards making the world a better place. While still constrained and predictable, it brings in new elements of motivation and historical context, showing a slight improvement in creativity compared to the first case.\n",
    "\n",
    "**To conclude :**\n",
    "\n",
    "Low temperature and low top-k configurations can be particularly useful for tasks where accurate, coherent, and predictable language is crucial, such as:\n",
    "\n",
    "- Machine Translation\n",
    "- Code Generation\n",
    "- Data-to-Text Tasks\n",
    "\n",
    "The generated text will be more deterministic, with the model selecting the most probable next token. The output will often be highly structured and coherent, sticking closely to the training data and avoiding significant deviations. The diversity of outputs is constrained, meaning that different runs with the same input prompt are likely to produce very similar results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f45b00",
   "metadata": {},
   "source": [
    "#### 1.2 High `temperature` and `top-k` Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b8f284bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very High temp, Very high top-k:\n",
      " Every effort moves you through life slowly but often not through punishment (usually life), through loss(less a blessing - much closer if desired.) Then once everything (usually personal goals etc ...). you start back backtrack, hoping for the \"best\", your dream \"will\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=5\n",
    ")\n",
    "\n",
    "print(\"Very High temp, Very high top-k:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1cbec37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High temp, high top-k:\n",
      " Every effort moves you, not me, into the abyss,\" said a man who asked for anonymity to talk candidly.\n",
      "\n",
      "The incident has prompted an investigation and an appeal for the police to investigate. Police are investigating.\n",
      "\n",
      "A spokesman said: \"We will\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=50,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=5,\n",
    "    temperature=2\n",
    ")\n",
    "\n",
    "print(\"High temp, high top-k:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d9ddc",
   "metadata": {},
   "source": [
    "In the \"Very High temp, Very High top-k\" output, the text is highly unpredictable and abstract, with a mix of vague and disjointed thoughts. It moves through topics like life, loss, and personal goals, but the coherence is weak, and the text feels more like a fragmented stream of consciousness. In contrast, the \"High temp, high top-k\" output is slightly more grounded but still maintains some level of unpredictability. It introduces a more direct narrative with a man speaking candidly, followed by a reference to a police investigation. This shift towards a more structured context gives the output more coherence while still keeping an element of surprise.\n",
    "\n",
    "\n",
    "*To conclude :**\n",
    "\n",
    "Higher temperature and top-k values are useful in domains where creativity, imagination, and novelty are valued over coherence and precision. These include:\n",
    "\n",
    "- Creative Writing\n",
    "- Poetry Generation\n",
    "- Story Generation\n",
    "\n",
    "The model will generate more diverse and creative text, sometimes introducing unexpected tokens. As randomness increases, the generated output may be less coherent and more disjointed. This is especially true with higher temperatures, where the model may select tokens that are less likely to fit in the context. The variety in the outputs increases significantly, making it more likely to generate unique and varied responses across multiple runs with the same prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12dc42d",
   "metadata": {},
   "source": [
    "# Exercise 5.3: Deterministic behavior in the decoding functions\n",
    "\n",
    "### What specific configuration parameters within the `generate` function can systematically eliminate randomness to ensure consistently reproducible generative outputs?\n",
    "\n",
    "#### 1. Random Seed Initialization:\n",
    "\n",
    "We need to fix the random seed first. Let's do that with : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d5625c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x207d2a1d530>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bd8e00",
   "metadata": {},
   "source": [
    "#### 2. Temperature scaling\n",
    "\n",
    "et the temperature to 0.0, which suppresses the softmax output's variability. Lowering the temperature makes the model output the token with the highest probability, completely removing randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4040fc7",
   "metadata": {},
   "source": [
    "#### 3. Top-k pruning mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f792ac",
   "metadata": {},
   "source": [
    "By using top_k=1, we ensure that only the highest probability token is sampled, further reducing randomness in the selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d949c",
   "metadata": {},
   "source": [
    "#### 4. Samplig strategy\n",
    "\n",
    "Use a greedy sampling strategy (i.e., always pick the token with the highest probability) instead of probabilistic methods such as multinomial sampling, which introduces variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024afc0a",
   "metadata": {},
   "source": [
    "##### 5. Conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c86e2e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very Low temp, Very Low top-k:\n",
      " Every effort moves you forward\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt, # model isn't well trained i don't run all epochs as i said before... \n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=1,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"Very Low temp, Very Low top-k:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d427a8",
   "metadata": {},
   "source": [
    "# Exercise 5.4: Continued pretraining\n",
    "\n",
    "### How can we effectively restore a machine learning model's training state across separate computational sessions, enabling seamless continuation of the pretraining process?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d0153c",
   "metadata": {},
   "source": [
    "#### 1. Load previously saved model weights\n",
    "\n",
    "To load the model weights from the previously saved checkpoint we can create teh function *load_model_weights*, as below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "05417f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, checkpoint_path=\"model.pth\"):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    # Load each layer's parameters directly (matching keys in the checkpoint)\n",
    "    model_state_dict = model.state_dict()\n",
    "    for key, value in checkpoint.items():\n",
    "        if key in model_state_dict:\n",
    "            model_state_dict[key] = value\n",
    "        else:\n",
    "            print(f\"Skipping parameter: {key}\")  # If the key doesn't match, it will be skipped\n",
    "    \n",
    "    model.load_state_dict(model_state_dict)\n",
    "    print(f\"Model weights loaded from {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb74757",
   "metadata": {},
   "source": [
    "#### 2. Reconstruct optimizer internal state\n",
    "\n",
    "To reconstruct the optimizer state and to continue training without losing progress we create the function *load_optimizer_state*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dcc928ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_optimizer_state(optimizer, checkpoint_path=\"model.pth\"):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    print(f\"Optimizer state restored from {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7111ecf",
   "metadata": {},
   "source": [
    "#### 3. Reinitiate training using `train_model_simple` function\n",
    "\n",
    "We can resume training using the train_model_simple function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "268fa307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulc\\AppData\\Local\\Temp\\ipykernel_17036\\3318896299.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping parameter: model_state_dict\n",
      "Skipping parameter: optimizer_state_dict\n",
      "Model weights loaded from model.pth\n",
      "Optimizer state restored from model.pth\n",
      "Ep 1 (Step 000000): Train loss 1.683, Val loss 1.922\n",
      "Ep 1 (Step 000005): Train loss 1.913, Val loss 1.921\n",
      "Ep 1 (Step 000010): Train loss 1.775, Val loss 1.932\n",
      "Ep 1 (Step 000015): Train loss 1.403, Val loss 1.916\n",
      "Ep 1 (Step 000020): Train loss 1.404, Val loss 1.904\n",
      "Ep 1 (Step 000025): Train loss 2.002, Val loss 1.924\n",
      "Ep 1 (Step 000030): Train loss 1.888, Val loss 1.904\n",
      "Ep 1 (Step 000035): Train loss 1.638, Val loss 1.886\n",
      "Ep 1 (Step 000040): Train loss 1.391, Val loss 1.847\n",
      "Ep 1 (Step 000045): Train loss 2.097, Val loss 1.857\n",
      "Ep 1 (Step 000050): Train loss 1.665, Val loss 1.833\n",
      "Ep 1 (Step 000055): Train loss 1.772, Val loss 1.800\n",
      "Ep 1 (Step 000060): Train loss 1.713, Val loss 1.794\n",
      "Ep 1 (Step 000065): Train loss 1.684, Val loss 1.759\n",
      "Ep 1 (Step 000070): Train loss 1.696, Val loss 1.752\n",
      "Ep 1 (Step 000075): Train loss 1.244, Val loss 1.769\n",
      "Ep 1 (Step 000080): Train loss 1.263, Val loss 1.759\n",
      "Ep 1 (Step 000085): Train loss 2.035, Val loss 1.757\n",
      "Ep 1 (Step 000090): Train loss 1.877, Val loss 1.752\n",
      "Ep 1 (Step 000095): Train loss 1.417, Val loss 1.768\n",
      "Ep 1 (Step 000100): Train loss 1.972, Val loss 1.787\n",
      "Ep 1 (Step 000105): Train loss 1.587, Val loss 1.768\n",
      "Ep 1 (Step 000110): Train loss 1.803, Val loss 1.770\n",
      "Ep 1 (Step 000115): Train loss 1.486, Val loss 1.767\n",
      "Ep 1 (Step 000120): Train loss 1.743, Val loss 1.755\n",
      "Ep 1 (Step 000125): Train loss 1.797, Val loss 1.755\n",
      "Every effort moves you know;quot;,&quot;,&quot;,&quot;,&quot;,&quot;,&quot;,&quot;,&quot;,&quot;\n"
     ]
    }
   ],
   "source": [
    "# Example of calling the training resumption function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Load model and optimizer from the checkpoint\n",
    "checkpoint_path = \"model.pth\"\n",
    "load_model_weights(model, checkpoint_path)\n",
    "load_optimizer_state(optimizer, checkpoint_path)\n",
    "\n",
    "# Resume training\n",
    "num_epochs = 1  # For one additional epoch\n",
    "train_losses, val_losses, track_tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq=5, eval_iter=5, \n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ac611",
   "metadata": {},
   "source": [
    "#### 4. Complete one additional training epoch\n",
    "\n",
    "Once the training is resumed, you can run one more epoch. This epoch will allow the model to continue learning from the last saved state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e116bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at checkpoint.pth\n"
     ]
    }
   ],
   "source": [
    "# After resuming training, you can save progress\n",
    "def save_checkpoint(model, optimizer, epoch, global_step, checkpoint_path=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"global_step\": global_step,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "    print(f\"Checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "# Save after completing the epoch\n",
    "\n",
    "epoch=1\n",
    "global_step=1\n",
    "\n",
    "save_checkpoint(model, optimizer, epoch + 1, global_step + len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d37bcd9",
   "metadata": {},
   "source": [
    "# Exercise 5.5: Training and validation set losses of the pretrained model\n",
    "\n",
    "### What are the comparative training and validation set losses when applying a pretrained OpenAI `GPTModel` to the \"The Verdict\" dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "19c42f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gpt.state_dict(), \"modelgpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "074e5feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulc\\AppData\\Local\\Temp\\ipykernel_17036\\3318896299.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from modelgpt.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#load_optimizer_state(optimizer_gpt, checkpoint_path_gpt)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Resume training\u001b[39;00m\n\u001b[0;32m     10\u001b[0m num_epochs_gpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# For one additional epoch\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_losses, val_losses, track_tokens_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_gpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs_gpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvery effort moves you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Save after completing the epoch\u001b[39;00m\n\u001b[0;32m     18\u001b[0m save_checkpoint(gpt, optimizer_gpt, epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, global_step \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader))\n",
      "Cell \u001b[1;32mIn[39], line 141\u001b[0m, in \u001b[0;36mtrain_model_simple\u001b[1;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Optional evaluation step\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_step \u001b[38;5;241m%\u001b[39m eval_freq \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m     train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    144\u001b[0m     val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss)\n",
      "Cell \u001b[1;32mIn[39], line 160\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, train_loader, val_loader, device, eval_iter)\u001b[0m\n\u001b[0;32m    158\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 160\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m calc_loss_loader(val_loader, model, device, num_batches\u001b[38;5;241m=\u001b[39meval_iter)\n\u001b[0;32m    162\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[1;32mIn[39], line 111\u001b[0m, in \u001b[0;36mcalc_loss_loader\u001b[1;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_batch, target_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m num_batches:\n\u001b[1;32m--> 111\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[39], line 94\u001b[0m, in \u001b[0;36mcalc_loss_batch\u001b[1;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_loss_batch\u001b[39m(input_batch, target_batch, model, device):\n\u001b[0;32m     93\u001b[0m     input_batch, target_batch \u001b[38;5;241m=\u001b[39m input_batch\u001b[38;5;241m.\u001b[39mto(device), target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 94\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), target_batch\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\paulc\\Documents\\Esiee_Paris\\E5\\p2\\data_engineering\\Paul\\previous_labs.py:208\u001b[0m, in \u001b[0;36mGPTModel.forward\u001b[1;34m(self, in_idx)\u001b[0m\n\u001b[0;32m    206\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_embeds \u001b[38;5;241m+\u001b[39m pos_embeds  \u001b[38;5;66;03m# Shape [batch_size, num_tokens, emb_size]\u001b[39;00m\n\u001b[0;32m    207\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_emb(x)\n\u001b[1;32m--> 208\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrf_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n\u001b[0;32m    210\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_head(x)\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\paulc\\Documents\\Esiee_Paris\\E5\\p2\\data_engineering\\Paul\\previous_labs.py:182\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    180\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m    181\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)\n\u001b[1;32m--> 182\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_shortcut(x)\n\u001b[0;32m    184\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m shortcut  \u001b[38;5;66;03m# Add the original input back\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\paulc\\Documents\\Esiee_Paris\\E5\\p2\\data_engineering\\Paul\\previous_labs.py:153\u001b[0m, in \u001b[0;36mFeedForward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\paulc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example of calling the training resumption function\n",
    "optimizer_gpt = torch.optim.Adam(gpt.parameters(), lr=1e-4)\n",
    "\n",
    "# Load model and optimizer from the checkpoint\n",
    "checkpoint_path_gpt = \"modelgpt.pth\"\n",
    "load_model_weights(gpt, checkpoint_path_gpt)\n",
    "#load_optimizer_state(optimizer_gpt, checkpoint_path_gpt)\n",
    "\n",
    "# Resume training\n",
    "num_epochs_gpt = 1  # For one additional epoch\n",
    "train_losses, val_losses, track_tokens_seen = train_model_simple(\n",
    "    gpt, train_loader, val_loader, optimizer_gpt, device, num_epochs_gpt, eval_freq=5, eval_iter=5, \n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "# Save after completing the epoch\n",
    "save_checkpoint(gpt, optimizer_gpt, epoch + 1, global_step + len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5429e9",
   "metadata": {},
   "source": [
    "# Exercise 5.6: Trying larger models\n",
    "\n",
    "### How do generative text characteristics vary across different GPT-2 model scales, specifically comparing the 124 million and 1,558 million parameter configurations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2-xl (1558M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt_big = GPTModel(NEW_CONFIG)\n",
    "gpt_big = GPTModel(NEW_CONFIG)\n",
    "gpt_big.eval();\n",
    "\n",
    "    \n",
    "load_weights_into_gpt(gpt_big, params)\n",
    "gpt_big.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadd170",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt, # model isn't well trained i don't run all epochs as i said before... \n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=1,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"gpt S\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83395fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = generate(\n",
    "    model=gpt_big, # model isn't well trained i don't run all epochs as i said before... \n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device),\n",
    "    max_new_tokens=1,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"gpt XL \", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
